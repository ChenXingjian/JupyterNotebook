{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Implementation of Group Normalization [CVPR2018 Kaiming He]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class GroupNorm(nn.Module):\n",
    "    def __init__(self, input_channels, group_number=32, eps=1e-5):\n",
    "        super(GroupNorm, self).__init__()\n",
    "        self.gamma=Parameter(torch.ones(1, input_channels, 1, 1))\n",
    "        self.beta=Parameter(torch.zeros(1, input_channels, 1, 1))\n",
    "        self.group_number=group_number\n",
    "        self.eps=eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W=x.size()\n",
    "        G=self.group_number\n",
    "        assert C % G == 0, 'Select a leagal G'\n",
    "        x1=x.view(N, G, -1)\n",
    "        x2=x.view(N, G, C//G, H, W)\n",
    "        \n",
    "        return x1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.7757]],\n",
      "\n",
      "         [[0.7641]],\n",
      "\n",
      "         [[0.5792]],\n",
      "\n",
      "         [[0.8568]]],\n",
      "\n",
      "\n",
      "        [[[0.5919]],\n",
      "\n",
      "         [[0.5885]],\n",
      "\n",
      "         [[0.0883]],\n",
      "\n",
      "         [[0.4775]]],\n",
      "\n",
      "\n",
      "        [[[0.9319]],\n",
      "\n",
      "         [[0.5619]],\n",
      "\n",
      "         [[0.8672]],\n",
      "\n",
      "         [[0.2979]]]])\n",
      "torch.Size([3, 2, 2])\n",
      "tensor([[[0.7757, 0.7641],\n",
      "         [0.5792, 0.8568]],\n",
      "\n",
      "        [[0.5919, 0.5885],\n",
      "         [0.0883, 0.4775]],\n",
      "\n",
      "        [[0.9319, 0.5619],\n",
      "         [0.8672, 0.2979]]])\n"
     ]
    }
   ],
   "source": [
    "input_feature=torch.rand((3, 4, 1, 1))\n",
    "print(input_feature)\n",
    "groupnorm=GroupNorm(4,2)\n",
    "output_feature=groupnorm(input_feature)\n",
    "print(output_feature.shape)\n",
    "print(output_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "[2] is not equal to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-edd76fa67024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assert expression for debug in python program\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[{0}] is not equal to 1'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: [2] is not equal to 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assert expression for debug in python program\n",
    "b=2\n",
    "assert b==1, '[{0}] is not equal to 1'.format(b)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Assert expression for debug in python program\n",
    "b=1\n",
    "assert b==1, '[{0}] is not equal to 1'.format(b)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0024, 0.0023],\n",
      "        [0.0013, 0.0179],\n",
      "        [0.0796, 0.0072]])\n",
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(3,2,2)\n",
    "var=torch.var(x,1)\n",
    "print(var)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class MIT5K(data.Dataset):\n",
    "    def __init__(self, root, train=True, img_size=256, is_transform=False):\n",
    "        super(MIT5K, self).__init__()\n",
    "        self.root=root\n",
    "        self.is_transform=is_transform\n",
    "        self.img_size=img_size\n",
    "        self.train=train\n",
    "        \n",
    "        if self.train:\n",
    "            self.subdir='fisheyeshangqi'\n",
    "        else:\n",
    "            self.subdir='val'\n",
    "            \n",
    "        self.files=os.listdir(os.path.join(self.root, self.subdir+'/image'))\n",
    "        \n",
    "    def img_transform(self, img, label):\n",
    "        img=img.astype(np.float32)\n",
    "        # BGR 2 RGB\n",
    "        img=img[:, :, ::-1]\n",
    "        label=label[:, :, ::-1]\n",
    "        # After img=cv2.imread(), img is a ndarray with shape [h,w,c], we need [c, h, w]\n",
    "        img=img.transpose(2, 0, 1)\n",
    "        label=label.transpose(2, 0, 1)\n",
    "        # If necessary, img should perform normalization: mean and std\n",
    "        return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "                            \n",
    "    def __getitem__(self, index):\n",
    "        # Get image path and label path\n",
    "        base_dir=os.path.join(self.root, self.subdir)\n",
    "        img_name=self.files[index]\n",
    "        img_path=base_dir + '/image/' + img_name\n",
    "        label_path=base_dir + '/label/' + img_name.split('.')[0] + '.png'\n",
    "        \n",
    "        # Read image and label\n",
    "        img=cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        label=cv2.imread(label_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        # Resize and Transform\n",
    "        img=cv2.resize(img, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "        label=cv2.resize(label, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "        img, label=self.img_transform(img, label)\n",
    "        \n",
    "        # Convert ndarray to torch.Tensor\n",
    "        # It's strange that must use img.copy() instead of img, or it will raise a error:\n",
    "        img=torch.from_numpy(img.copy()).float()\n",
    "        label=torch.from_numpy(label.copy()).float()\n",
    "        return img, label\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "0 torch.Size([16, 3, 256, 256]) torch.Size([16, 3, 256, 256])\n",
      "1 torch.Size([16, 3, 256, 256]) torch.Size([16, 3, 256, 256])\n",
      "2 torch.Size([16, 3, 256, 256]) torch.Size([16, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "mitdataset=MIT5K('/home/philochan/dataset')\n",
    "print(len(mitdataset))\n",
    "mitloader=data.DataLoader(mitdataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "for i,(img, target) in enumerate(mitloader):\n",
    "    if i==3:\n",
    "        break\n",
    "    print(i, img.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/philochan/dataset/fisheyeshangqi/image\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "base_dir=os.path.join('/home/philochan/dataset/fisheyeshangqi', 'image')\n",
    "print(base_dir)\n",
    "a=os.listdir(base_dir)\n",
    "#for ele in a:\n",
    "#    print(ele.split('.')[0])\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1cdb9e8ec279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestimg_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'F000001.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtestimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(type(testimg))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#----------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_dir' is not defined"
     ]
    }
   ],
   "source": [
    "testimg_path=os.path.join(base_dir,'F000001.jpg')\n",
    "print(testimg_path)\n",
    "testimg=cv2.imread(testimg_path)\n",
    "#print(type(testimg))\n",
    "#----------------------------------\n",
    "#testimg=testimg.astype(np.float32)\n",
    "# BGR 2 RGB\n",
    "testimg=testimg[:, :, ::-1]\n",
    "cv2.imshow('img',testimg)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_reconstruction(inputs, targets, S):\n",
    "    \"\"\" reconstruction loss\n",
    "    Arguments:\n",
    "    inputs: [N, 3, H, W]\n",
    "    targets:[N, 3, H, W]\n",
    "    S:      [N, 3, H, W]\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def L_color(inputs, targets, S):\n",
    "    \"\"\" color loss\n",
    "    Arguments:\n",
    "    inputs:  [N, 3, H, W]\n",
    "    targets: [N, 3, H, W]\n",
    "    S:       [N, 3, H, W]\n",
    "    \"\"\"\n",
    "    predictions = 1 / S * inputs\n",
    "    predictions = F.normalize(predictions, p=2, dim=1)\n",
    "    targets = F.normalize(targets, p=2, dim=1)\n",
    "    cosine_value = predictions * targets\n",
    "    angle = torch.acos(cosine_value)\n",
    "    return torch.sum(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_smoothness(inputs, S):\n",
    "    \"\"\" smoothness loss\n",
    "    Arguments:\n",
    "    inputs:  [N, 3, H, W]\n",
    "    S:       [N, 3, H, W]\n",
    "    \"\"\"\n",
    "    theta=1.2\n",
    "    epsilon=0.0001\n",
    "    # torch.log() compute natural logarithm\n",
    "    Log_images=torch.log(inputs)\n",
    "    # Define gradient operator\n",
    "    conv_x = nn.Conv2d(3, 3, (1, 3), padding=(0,1), groups=3, bias=False)\n",
    "    conv_y = nn.Conv2d(3, 3, (3, 1), padding=(1,0), groups=3, bias=False)\n",
    "    grad_x_kernel = np.array([[[[-1, 0, 1]]], [[[-1, 0,1]]], [[[-1, 0, 1]]]], dtype='float32')\n",
    "    conv_x.weight.data = torch.from_numpy(grad_x_kernel)\n",
    "    grad_y_kernel = np.array([[[[-1], [0], [1]]], [[[-1], [0],[1]]], [[[-1], [0], [1]]]], dtype='float32')\n",
    "    conv_y.weight.data = torch.from_numpy(grad_y_kernel)\n",
    "    # Compute gradient image and weights\n",
    "    Lx = torch.abs(conv_x(Log_images))\n",
    "    Ly = torch.abs(conv_y(Log_images))\n",
    "    Wx = 1.0 / (Lx ** theta + epsilon)\n",
    "    Wy = 1.0 / (Ly ** theta + epsilon)\n",
    "    # Compute gradient image of illumination map S\n",
    "    Sx_2 = conv_x(S) ** 2\n",
    "    Sy_2 = conv_y(S) ** 2\n",
    "    return torch.sum(Wx * Sx_2 + Wy * Sy_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5703.8359, grad_fn=<SumBackward0>)\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "tin1=torch.rand((3,3,8,8))\n",
    "tin2=torch.rand((3,3,8,8))\n",
    "lossm=L_smoothness(tin1, tin2)\n",
    "print(lossm)\n",
    "print(lossm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1=torch.rand((5,2,1,1))\n",
    "#print(t1)\n",
    "t2= 1 / t1\n",
    "#print(t2)\n",
    "t3=t1 * t2\n",
    "#print(t3)\n",
    "print(t3[0, :, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3026)\n"
     ]
    }
   ],
   "source": [
    "print(torch.log(torch.tensor(10.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "img_path='/home/philochan/lena.jpeg'\n",
    "image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "laplacian_kernel= torch.tensor([[[0,1,0],[1,-4,1],[0,1,0]], [[0,1,0],[1,-4,1],[0,1,0]], [[0,1,0],[1,-4,1],[0,1,0]]])\n",
    "print(laplacian_kernel.shape)\n",
    "output_img=torch.nn.Conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='https://data.csail.mit.edu/graphics/fivek/img/tiff16_c/'\n",
    "with open('imagename.txt','r') as f1:\n",
    "    for line in f1.readlines():\n",
    "        #用line.strip()把最后的换行符去掉\n",
    "        image_name=line.strip() + '.tif'\n",
    "        image_path=prefix + image_name\n",
    "        with open('download_label.txt', 'a') as f2:\n",
    "            f2.write(image_path)\n",
    "            f2.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 1, 3])\n",
      "torch.Size([3, 1, 3, 1])\n",
      "torch.Size([3, 1, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "inputs=torch.rand((1, 3, 8, 8))\n",
    "conv_x = nn.Conv2d(3, 3, (1, 3), padding=(0,1), groups=3, bias=False)\n",
    "print(conv_x.weight.data.shape)\n",
    "grad_x_kernel = np.array([[[[-1, 0, 1]]], [[[-1, 0,1]]], [[[-1, 0, 1]]]], dtype='float32')\n",
    "conv_x.weight.data = torch.from_numpy(grad_x_kernel)\n",
    "\n",
    "conv_y = nn.Conv2d(3, 3, (3, 1), padding=(1,0), groups=3, bias=False)\n",
    "grad_y_kernel =torch.from_numpy(np.array([[[[-1], [0], [1]]], [[[-1], [0],[1]]], [[[-1], [0], [1]]]], dtype='float32'))\n",
    "results = conv_x(inputs)\n",
    "print(grad_y_kernel.shape)\n",
    "#print(inputs)\n",
    "#print(results)\n",
    "print(conv_y.weight.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0a0+90737f7'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoNormalization(x):\n",
    "    max_num=torch.max(x)\n",
    "    min_num=torch.min(x)\n",
    "    return (x - min_num) / (max_num - min_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_reconstruction(inputs, targets, S):\n",
    "    inputs=DoNormalization(inputs)\n",
    "    targets=DoNormalization(targets)\n",
    "    basic_loss=torch.sum((inputs - S * targets) ** 2)\n",
    "    OneTensor=torch.ones_like(S)\n",
    "    penalty_loss=torch.sum(F.relu(inputs-S)) + torch.sum(F.relu(S-OneTensor))\n",
    "    penalty_param=10000\n",
    "    return basic_loss + penalty_param * penalty_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(958715.8125)\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "tin1=torch.rand((3,3,8,8))\n",
    "tin3=torch.rand((3,3,8,8))\n",
    "tin2=torch.rand((3,3,8,8))\n",
    "lossm=L_reconstruction(tin1, tin2, tin3)\n",
    "print(lossm)\n",
    "print(lossm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]\n",
      "Epoch: [1]\n",
      "Epoch: [2]\n",
      "Epoch: [3]\n",
      "Epoch: [4]\n",
      "Epoch: [5]\n",
      "Epoch: [6]\n",
      "Epoch: [7]\n",
      "Epoch: [8]\n",
      "Epoch: [9]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print('Epoch: [{}]'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.300000190734863\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([5.3])\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch预训练模型\n",
    "import torchvision.models as models\n",
    "# 加载网络结构和预训练参数\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "# 只加载网络结构，不加载预训练参数\n",
    "resnet18 = models.resnet18(pretrained=False)\n",
    "# 打印模型的网络结构\n",
    "print(resnet18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
